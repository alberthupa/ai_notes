<!DOCTYPE html>
<html>
<head>
<title>vision_ai.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="vision-ai">VISION AI</h1>
<ul>
<li>
<p><strong>Computer Vision as a Service (CVaaS)</strong>: Many companies are offering cloud-based computer vision services that allow businesses to integrate advanced image recognition capabilities into their applications without needing in-depth AI expertise. These services include image classification, object detection, and facial recognition, among others.</p>
</li>
<li>
<p><strong>Automated Quality Inspection</strong>: In manufacturing, AI vision is being used for quality control processes. These systems can detect defects or irregularities in products at a speed and accuracy level that far exceeds human capabilities. This not only reduces costs but also improves product quality.</p>
</li>
<li>
<p><strong>Retail Analytics</strong>: AI vision technologies are being used in the retail sector for customer behavior analysis, inventory management, and enhancing the shopping experience through personalized recommendations. This includes tracking customer movements within stores to analyze traffic patterns and product interactions.</p>
</li>
<li>
<p><strong>Healthcare Diagnostics</strong>: AI vision is revolutionizing healthcare by assisting in the diagnosis of diseases from medical imagery such as X-rays, MRIs, and CT scans with greater accuracy and speed. This includes detecting cancers at earlier stages, thus significantly improving patient outcomes.</p>
</li>
<li>
<p><strong>Augmented Reality (AR)</strong> and Virtual Reality (VR): With the integration of AI vision, AR and VR technologies are becoming more interactive and immersive. This is used in various applications, from gaming and entertainment to education and training simulations.</p>
</li>
<li>
<p><strong>Generative AI for Content Creation</strong>: Beyond analysis, AI vision is also being used to generate visual content. This includes creating realistic images, videos, and simulations for entertainment, marketing, and training purposes. Generative Adversarial Networks (GANs) are a key technology here.</p>
</li>
<li>
<p><strong>Edge AI in Vision</strong>: There's a growing trend towards processing AI tasks at the edge, i.e., on local devices, rather than in the cloud. This reduces latency, increases privacy, and lowers bandwidth requirements for applications like real-time video analytics and IoT devices.</p>
</li>
<li>
<p><strong>Autonomous Vehicles</strong>: AI vision is a critical component of autonomous vehicle technology, enabling vehicles to interpret and understand the environment around them to navigate safely. This includes object and pedestrian detection, traffic sign recognition, and real-time decision-making.</p>
</li>
<li>
<p><strong>Ethical and Responsible AI</strong>: As AI vision technologies become more pervasive, there's an increased focus on ethical considerations, including privacy, bias, and accountability. Consultancy companies are now offering services to help businesses deploy AI vision solutions in an ethical and responsible manner.</p>
</li>
<li>
<p><strong>Custom AI Models</strong>: While pre-trained models offer a great starting point, there's a trend towards custom AI models tailored to specific business needs and data. Consultancies are helping companies develop these custom models to maximize performance and relevance to their unique challenges.</p>
</li>
</ul>
<h1 id="pretrained-models">PRETRAINED MODELS</h1>
<h2 id="models-on-huggingface">MODELS ON HUGGINGFACE</h2>
<ul>
<li>Models on Hugging Face include image classification, object detection, semantic segmentation, and more.</li>
<li>Some of the popular architectures available include Convolutional Neural Networks (CNNs) like ResNet, EfficientNet, and Vision Transformers (ViT).</li>
</ul>
<h3 id="typical-usage-of-pretrained-models">Typical Usage of Pretrained Models</h3>
<ul>
<li>
<p><strong>Image Classification</strong>: One of the most common uses of pretrained models is to categorize images into predefined classes. Models trained on large datasets like ImageNet are often used as starting points for specific image classification tasks.</p>
</li>
<li>
<p><strong>Object Detection</strong>: Pretrained models are also used to identify and locate objects within images. This is crucial for applications like surveillance, vehicle navigation, and retail analytics.</p>
</li>
<li>
<p><strong>Semantic Segmentation</strong>: In tasks where understanding the context of each pixel in an image is necessary, semantic segmentation models come into play. They are widely used in medical imaging, autonomous vehicles, and land use/land cover mapping in satellite imagery.</p>
</li>
<li>
<p><strong>Transfer Learning</strong>: A significant use case for pretrained models is transfer learning, where a model trained on one task is fine-tuned for another, related task. This approach allows for leveraging the knowledge gained from large, diverse datasets, even when the target task has limited data available.</p>
</li>
<li>
<p><strong>Feature Extraction</strong>: Pretrained models are also used as feature extractors. In this scenario, the output of one of the intermediate layers of the model is used as a compact representation of the input image. These features can then be used for various tasks, such as similarity search or clustering.</p>
</li>
<li>
<p>[](</p>
</li>
<li>
<p>[](https://huggingface.co/runwayml/stable-diffusion-v1-5   10300</p>
</li>
<li>
<p>[](https://huggingface.co/CompVis/stable-diffusion-v1-4    6210
https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0 4510
https://huggingface.co/WarriorMama777/OrangeMixs    3590
https://huggingface.co/prompthero/openjourney   3001
https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt 1790
https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0  1360
https://huggingface.co/openai/clip-vit-large-patch14    932
https://huggingface.co/timbrooks/instruct-pix2pix   786
https://huggingface.co/Salesforce/blip-image-captioning-large   706
https://huggingface.co/nlpconnect/vit-gpt2-image-captioning 683
https://huggingface.co/briaai/RMBG-1.4  601
https://huggingface.co/stabilityai/stable-video-diffusion-img2vid   557
https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace   510
https://huggingface.co/google/vit-base-patch16-224  453
https://huggingface.co/stabilityai/stable-zero123   432
https://huggingface.co/ali-vilab/text-to-video-ms-1.7b  431
https://huggingface.co/cerspense/zeroscope_v2_576w  420
https://huggingface.co/facebook/detr-resnet-50  418
https://huggingface.co/Salesforce/blip-image-captioning-base    337
https://huggingface.co/lambdalabs/sd-image-variations-diffusers 330
https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K    239</p>
</li>
</ul>
<h2 id="synthetic-datasets">SYNTHETIC DATASETS</h2>
<p>Synthetic data generation involves creating artificial images or videos that mimic real-world data. This technique is particularly useful in scenarios where collecting real-world data is challenging, expensive, or constrained by privacy and ethical considerations.</p>
<h3 id="how-synthetic-datasets-are-generated">How Synthetic Datasets Are Generated</h3>
<ul>
<li><strong>3D Rendering</strong>: Using 3D modeling software to create scenes and objects that are then rendered into images or videos. This method is commonly used for generating datasets for autonomous vehicle training, where diverse traffic scenarios can be simulated.</li>
<li><strong>Generative Adversarial Networks (GANs)</strong>: GANs can generate highly realistic images by learning the distribution of a dataset. They are particularly useful for augmenting datasets with rare but critical cases, such as medical imaging datasets with rare diseases.</li>
<li><strong>Data Augmentation Techniques</strong>: While not generating completely new data, augmentation techniques such as flipping, rotation, scaling, and color variation can create variations of existing data, effectively increasing the size and diversity of a dataset.</li>
<li><strong>Simulation Environments</strong>: Simulated environments, often used in robotics and autonomous vehicles, can generate vast amounts of synthetic data. These environments simulate real-world physics and interactions, providing a rich source of diverse data.</li>
</ul>

</body>
</html>
